{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer    \n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten, Conv1D, MaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incializando as stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"de\",\"a\",\"o\",\"que\",\"e\",\"do\",\"da\",\"em\",\"um\",\"para\",\"é\",\"com\",\"uma\",\"os\",\"no\",\"se\",\"na\",\"por\",\"mais\",\"as\",\"dos\",\"como\",\"mas\",\"foi\",\"ao\",\"ele\",\"das\",\"tem\",\"à\",\"seu\",\"sua\",\"ou\",\"ser\",\"quando\",\"muito\",\"há\",\"nos\",\"já\",\"está\",\"eu\",\"também\",\"só\",\"pelo\",\"pela\",\"até\",\"isso\",\"ela\",\"entre\",\"era\",\"depois\",\"sem\",\"mesmo\",\"aos\",\"ter\",\"seus\",\"quem\",\"nas\",\"me\",\"esse\",\"eles\",\"estão\",\"você\",\"tinha\",\"foram\",\"essa\",\"num\",\"nem\",\"suas\",\"meu\",\"às\",\"minha\",\"têm\",\"numa\",\"pelos\",\"elas\",\"havia\",\"seja\",\"qual\",\"será\",\"nós\",\"tenho\",\"lhe\",\"deles\",\"essas\",\"esses\",\"pelas\",\"este\",\"fosse\",\"dele\",\"tu\",\"te\",\"vocês\",\"vos\",\"lhes\",\"meus\",\"minhas\",\"teu\",\"tua\",\"teus\",\"tuas\",\"nosso\",\"nossa\",\"nossos\",\"nossas\",\"dela\",\"delas\",\"esta\",\"estes\",\"estas\",\"aquele\",\"aquela\",\"aqueles\",\"aquelas\",\"isto\",\"aquilo\",\"estou\",\"está\",\"estamos\",\"estão\",\"estive\",\"esteve\",\"estivemos\",\"estiveram\",\"estava\",\"estávamos\",\"estavam\",\"estivera\",\"estivéramos\",\"esteja\",\"estejamos\",\"estejam\",\"estivesse\",\"estivéssemos\",\"estivessem\",\"estiver\",\"estivermos\",\"estiverem\",\"hei\",\"há\",\"havemos\",\"hão\",\"houve\",\"houvemos\",\"houveram\",\"houvera\",\"houvéramos\",\"haja\",\"hajamos\",\"hajam\",\"houvesse\",\"houvéssemos\",\"houvessem\",\"houver\",\"houvermos\",\"houverem\",\"houverei\",\"houverá\",\"houveremos\",\"houverão\",\"houveria\",\"houveríamos\",\"houveriam\",\"sou\",\"somos\",\"são\",\"era\",\"éramos\",\"eram\",\"fui\",\"foi\",\"fomos\",\"foram\",\"fora\",\"fôramos\",\"seja\",\"sejamos\",\"sejam\",\"fosse\",\"fôssemos\",\"fossem\",\"for\",\"formos\",\"forem\",\"serei\",\"será\",\"seremos\",\"serão\",\"seria\",\"seríamos\",\"seriam\",\"tenho\",\"tem\",\"temos\",\"tém\",\"tinha\",\"tínhamos\",\"tinham\",\"tive\",\"teve\",\"tivemos\",\"tiveram\",\"tivera\",\"tivéramos\",\"tenha\",\"tenhamos\",\"tenham\",\"tivesse\",\"tivéssemos\",\"tivessem\",\"tiver\",\"tivermos\",\"tiverem\",\"terei\",\"terá\",\"teremos\",\"terão\",\"teria\",\"teríamos\",\"teriam\", \"-\", \"_\", \"\\“\", \"\\\"\", \"|\", \"/\", \":\", \",\", \".\", \"?\", \"!\", \"*\", \"(\", \")\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lendo os arquivos e retirando as stopwords com pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cloroquina = pd.read_excel(\"ep2-cloroquina-treino.xlsx\",sheet_name='train',usecols=['texto','posicao'])\n",
    "x = df_cloroquina.texto.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stopwords))\n",
    "y = df_cloroquina.posicao\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformando os dados textuais em sequências númericas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=11000)\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "x_train_numerico = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_numerico = tokenizer.texts_to_sequences(x_test)\n",
    "tamanho_vocabulario = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformando os dados de treino e teste em entradas para a rede neural através de padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanho_embedding = 300\n",
    "\n",
    "x_train_padded = pad_sequences(x_train_numerico, padding='post', maxlen=tamanho_embedding)\n",
    "x_test_padded = pad_sequences(x_test_numerico, padding='post', maxlen=tamanho_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando a lista de palavras (token) de cada linha de dado do arquivo de dados, o TfidfVectorizer está sendo usado como filtro de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=[1,1], analyzer='word', token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "\n",
    "analyzer = tfidf.build_analyzer()\n",
    "lista_tokens = []\n",
    "\n",
    "for item in x:\n",
    "    tokens = analyzer(item)\n",
    "    lista_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando embeddings com vetores de tamanho 300 com a lista de tokens criada anteriormente. Logo após, utilizando a matriz de pesos do modelo Word2Vec para criar uma camada de embeddings para a rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(lista_tokens, vector_size=tamanho_embedding, workers=8, min_count=1)\n",
    "num_palavras = word2vec.wv.vectors.shape[0]\n",
    "embedding_layer = Embedding(num_palavras, tamanho_embedding, weights=[word2vec.wv.vectors], input_length=tamanho_embedding, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constroi uma rede neural CNN utilizando a biblioteca Keras, incorpora o embedding criado com Word2Vec na rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constroi_modelo_cnn():\n",
    "    modelo = Sequential()\n",
    "    \n",
    "    modelo.add(embedding_layer)\n",
    "    modelo.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    modelo.add(MaxPooling1D(pool_size=2))\n",
    "    #Dropouts reduzem o overfitting da rede ao desativar neurônios durante o treinamento.\n",
    "    modelo.add(Dropout(0.2))\n",
    "    modelo.add(Flatten())\n",
    "    modelo.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    modelo.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria uma função de parada antecipada para evitar aumento da função loss da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforma o modelo neural do Keras em um modelo que pode ser entendido pelo Sklearn (wrapper) e inicializa o treinamento e validações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rede_neural = KerasClassifier(build_fn=constroi_modelo_cnn, epochs=11)\n",
    "kfolds = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "resultado = cross_val_score(rede_neural, x_train_padded, y_train, cv=kfolds, scoring='accuracy', fit_params={'callbacks': [early_stopping]}).mean()\n",
    "print(\"Acuracia de treino: \" + resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testa a acuracia da rede com os dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rede_neural.fit(x_test_padded, y_test)\n",
    "predicao = rede_neural.predict(x_test_padded)\n",
    "acuracia = accuracy_score(predicao, y_test)\n",
    "print(\"Acuracia de teste: \" + acuracia)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4fc3c70bdf126d36ef05241663b6e1e6a74791aef0a540d9607b18a3d8930c22"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
